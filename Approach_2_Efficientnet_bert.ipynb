{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    TRAIN_CSV_PATH = 'dataset/train.csv'\n",
    "    TEST_CSV_PATH = 'dataset/test.csv'\n",
    "    TRAIN_IMG_DIR = './src/complete_images/train_images/'\n",
    "    TEST_IMG_DIR = './src/complete_images/test_images/'\n",
    "    SUBMISSION_PATH = 'submission_efficientnet_bert.csv'\n",
    "    MODEL_SAVE_PATH = 'best_efficientnet_bert_model.pth'\n",
    "    \n",
    "    # Model parameters\n",
    "    IMAGE_MODEL = \"efficientnet_b4\"  # ~19M params\n",
    "    TEXT_MODEL = \"bert-base-uncased\"  # ~110M params\n",
    "    IMAGE_SIZE = 380\n",
    "    MAX_TEXT_LENGTH = 256 \n",
    "    \n",
    "    BATCH_SIZE = 35  \n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE_IMG = 1e-4\n",
    "    LEARNING_RATE_TEXT = 2e-5\n",
    "    LEARNING_RATE_HEAD = 1e-3\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    DROPOUT = 0.35\n",
    "    \n",
    "    VAL_SPLIT = 0.15\n",
    "    WARMUP_EPOCHS = 4\n",
    "    \n",
    "    USE_AUGMENTATION = True\n",
    "    \n",
    "    PATIENCE = 8\n",
    "    \n",
    "    USE_AMP = True\n",
    "    \n",
    "    SEED = 123  \n",
    "    NUM_WORKERS = 4\n",
    "\n",
    "def set_seed(seed=123):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "\n",
    "def get_train_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "class ProductPriceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_dir, mode='train', use_augmentation=None, testing=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_dir = image_dir\n",
    "        self.mode = mode\n",
    "        self.testing = testing  \n",
    "        if use_augmentation is None:\n",
    "            use_augmentation = (mode == 'train')\n",
    "        \n",
    "        if use_augmentation and mode == 'train':\n",
    "            self.transform = get_train_transforms()\n",
    "        else:\n",
    "            self.transform = get_val_transforms()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]  \n",
    "        \n",
    "        sample_id = row['sample_id']\n",
    "        \n",
    "        try:\n",
    "            if self.testing: \n",
    "                image_filename = f\"test_{sample_id}.jpg\"\n",
    "            else:\n",
    "                image_filename = f\"train_{sample_id}.jpg\"\n",
    "            \n",
    "            image_path = os.path.join(self.image_dir, image_filename)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                for ext in ['.png', '.jpeg', '.JPG', '.PNG']:\n",
    "                    if self.testing: \n",
    "                        alt_path = os.path.join(self.image_dir, f\"test_{sample_id}{ext}\")\n",
    "                    else:\n",
    "                        alt_path = os.path.join(self.image_dir, f\"train_{sample_id}{ext}\")\n",
    "                    if os.path.exists(alt_path):\n",
    "                        image_path = alt_path\n",
    "                        break\n",
    "            \n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image)  \n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (224, 224), color=(128, 128, 128))\n",
    "            image = self.transform(image) \n",
    "            print(f\"Warning: Could not load image for {sample_id}, using blank image\")\n",
    "        \n",
    "        text = str(row['catalog_content'])\n",
    "        if pd.isna(text) or text == 'nan':\n",
    "            text = \"No description available\"\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=Config.MAX_TEXT_LENGTH,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        output = {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'sample_id': sample_id\n",
    "        }\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            price = float(row['price'])\n",
    "            log_price = np.log1p(price)\n",
    "            output['price'] = torch.tensor(log_price, dtype=torch.float32)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class DualEncoderPricePredictor(nn.Module):\n",
    "    def __init__(self, image_model_name=Config.IMAGE_MODEL, text_model_name=Config.TEXT_MODEL, dropout=Config.DROPOUT):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"Loading image model: {image_model_name}\")\n",
    "        if image_model_name == \"efficientnet_b4\":\n",
    "            self.image_encoder = models.efficientnet_b4(pretrained=True)\n",
    "            image_feat_dim = 1792  # EfficientNet-B4 output\n",
    "            self.image_encoder.classifier = nn.Identity()\n",
    "        elif image_model_name == \"efficientnet_b3\":\n",
    "            self.image_encoder = models.efficientnet_b3(pretrained=True)\n",
    "            image_feat_dim = 1536\n",
    "            self.image_encoder.classifier = nn.Identity()\n",
    "        \n",
    "        print(f\"Loading text model: {text_model_name}\")\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_feat_dim = 768 \n",
    "        \n",
    "        common_dim = 512\n",
    "        self.image_projection = nn.Sequential(\n",
    "            nn.Linear(image_feat_dim, common_dim),\n",
    "            nn.LayerNorm(common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(text_feat_dim, common_dim),\n",
    "            nn.LayerNorm(common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=common_dim,\n",
    "            num_heads=8,\n",
    "            dropout=dropout * 0.5,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(common_dim * 3, 768), \n",
    "            nn.LayerNorm(768),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(768, 384),\n",
    "            nn.LayerNorm(384),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(384, 192),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            \n",
    "            nn.Linear(192, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in [self.image_projection, self.text_projection, self.fusion_head]:\n",
    "            for m in module.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        image_features = self.image_encoder(images) \n",
    "        image_proj = self.image_projection(image_features)  \n",
    "        \n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]  \n",
    "        text_proj = self.text_projection(text_features)  \n",
    "        \n",
    "        image_proj_unsqueezed = image_proj.unsqueeze(1)  \n",
    "        text_proj_unsqueezed = text_proj.unsqueeze(1)\n",
    "        \n",
    "        attended_features, _ = self.cross_attention(\n",
    "            image_proj_unsqueezed,\n",
    "            text_proj_unsqueezed,\n",
    "            text_proj_unsqueezed\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(1) \n",
    "        \n",
    "        combined = torch.cat([image_proj, text_proj, attended_features], dim=1)\n",
    "        \n",
    "        log_price = self.fusion_head(combined)\n",
    "        return log_price.squeeze(-1)\n",
    "\n",
    "class SMAPELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred_original = torch.expm1(pred)\n",
    "        target_original = torch.expm1(target)\n",
    "        \n",
    "        numerator = torch.abs(pred_original - target_original)\n",
    "        denominator = (torch.abs(target_original) + torch.abs(pred_original)) / 2 + self.epsilon\n",
    "        smape = torch.mean(numerator / denominator)\n",
    "        \n",
    "        return smape\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, scaler=None, epoch=0, warmup_epochs=Config.WARMUP_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    if epoch < warmup_epochs:\n",
    "        for param in model.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.image_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.text_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        log_prices = batch['price'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler and Config.USE_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(images, input_ids, attention_mask)\n",
    "                loss = criterion(predictions, log_prices)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            predictions = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(predictions, log_prices)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            log_prices = batch['price'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(log_prices.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    preds_original = np.expm1(all_preds)\n",
    "    targets_original = np.expm1(all_targets)\n",
    "    preds_original = np.maximum(preds_original, 0.01)\n",
    "    \n",
    "    smape = np.mean(np.abs(preds_original - targets_original) / \n",
    "                    ((np.abs(targets_original) + np.abs(preds_original)) / 2 + 1e-8)) * 100\n",
    "    \n",
    "    mae = np.mean(np.abs(preds_original - targets_original))\n",
    "    rmse = np.sqrt(np.mean((preds_original - targets_original) ** 2))\n",
    "    \n",
    "    return smape, mae, rmse\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_sample_ids = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            sample_ids = batch['sample_id']\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                log_predictions = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            predictions = torch.expm1(log_predictions)\n",
    "            predictions = torch.clamp(predictions, min=0.01)\n",
    "            \n",
    "            all_sample_ids.extend(sample_ids)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    return all_sample_ids, all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EfficientNet-BERT Product Price Prediction\n",
      "Alternative Approach for Ensemble\n",
      "============================================================\n",
      "Number of GPUs available: 2\n",
      "Using device: cuda:1\n",
      "GPU name: Tesla V100S-PCIE-32GB\n",
      "GPU: Tesla V100S-PCIE-32GB\n",
      "GPU Memory: 34.08 GB\n",
      "\n",
      "Loading training data from dataset/train.csv\n",
      "Training samples: 75000\n",
      "\n",
      "Price Statistics:\n",
      "Min: $0.13\n",
      "Max: $2796.00\n",
      "Mean: $23.65\n",
      "Median: $14.00\n",
      "\n",
      "Train set: 63750 samples\n",
      "Validation set: 11250 samples\n",
      "\n",
      "Initializing tokenizer and model...\n",
      "Loading image model: efficientnet_b4\n",
      "Loading text model: bert-base-uncased\n",
      "Total parameters: 130.95M\n",
      "Trainable parameters: 130.95M\n",
      "\n",
      "Creating datasets with augmentation...\n",
      "\n",
      "Starting training for 15 epochs...\n",
      "Warmup epochs: 4\n",
      "Image size: 380x380\n",
      "Data augmentation: True\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  37%|███▋      | 682/1821 [12:22<13:34,  1.40it/s, loss=0.6421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1821/1821 [34:08<00:00,  1.12s/it, loss=0.6925]\n",
      "Validating: 100%|██████████| 322/322 [05:49<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6814\n",
      "Val SMAPE: 66.55%\n",
      "Val MAE: $15.62\n",
      "Val RMSE: $28.37\n",
      "✓ Model saved! Best SMAPE: 66.55%\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  71%|███████▏  | 1302/1821 [25:11<10:32,  1.22s/it, loss=0.7617]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1821/1821 [43:09<00:00,  1.42s/it, loss=0.5610]\n",
      "Validating: 100%|██████████| 322/322 [06:08<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6297\n",
      "Val SMAPE: 59.63%\n",
      "Val MAE: $13.77\n",
      "Val RMSE: $28.05\n",
      "✓ Model saved! Best SMAPE: 59.63%\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  62%|██████▏   | 1134/1821 [24:57<12:28,  1.09s/it, loss=0.6186] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1821/1821 [39:07<00:00,  1.29s/it, loss=0.4772]\n",
      "Validating: 100%|██████████| 322/322 [05:51<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6135\n",
      "Val SMAPE: 58.75%\n",
      "Val MAE: $13.59\n",
      "Val RMSE: $28.55\n",
      "✓ Model saved! Best SMAPE: 58.75%\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  90%|█████████ | 1647/1821 [40:40<02:20,  1.24it/s, loss=0.6156] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1821/1821 [44:44<00:00,  1.47s/it, loss=0.6593]\n",
      "Validating: 100%|██████████| 322/322 [05:25<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6035\n",
      "Val SMAPE: 58.78%\n",
      "Val MAE: $13.51\n",
      "Val RMSE: $28.33\n",
      "Patience: 1/8\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:   9%|▊         | 157/1821 [03:26<30:54,  1.11s/it, loss=0.5662] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 1821/1821 [37:21<00:00,  1.23s/it, loss=0.5122]  \n",
      "Validating: 100%|██████████| 322/322 [07:34<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5648\n",
      "Val SMAPE: 50.54%\n",
      "Val MAE: $11.47\n",
      "Val RMSE: $24.13\n",
      "✓ Model saved! Best SMAPE: 50.54%\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6:  69%|██████▉   | 1265/1821 [35:21<10:24,  1.12s/it, loss=0.5208] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 1821/1821 [51:13<00:00,  1.69s/it, loss=0.4571]\n",
      "Validating: 100%|██████████| 322/322 [05:12<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5028\n",
      "Val SMAPE: 48.04%\n",
      "Val MAE: $11.08\n",
      "Val RMSE: $23.30\n",
      "✓ Model saved! Best SMAPE: 48.04%\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:   5%|▌         | 97/1821 [01:28<18:18,  1.57it/s, loss=0.3320] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 1821/1821 [44:12<00:00,  1.46s/it, loss=0.6107] \n",
      "Validating: 100%|██████████| 322/322 [05:37<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4649\n",
      "Val SMAPE: 48.04%\n",
      "Val MAE: $10.91\n",
      "Val RMSE: $22.46\n",
      "Patience: 1/8\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8:  50%|█████     | 911/1821 [19:09<24:56,  1.64s/it, loss=0.5060] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 1821/1821 [38:02<00:00,  1.25s/it, loss=0.5030]\n",
      "Validating: 100%|██████████| 322/322 [05:30<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4352\n",
      "Val SMAPE: 45.71%\n",
      "Val MAE: $10.32\n",
      "Val RMSE: $22.84\n",
      "✓ Model saved! Best SMAPE: 45.71%\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9:  98%|█████████▊| 1778/1821 [40:21<00:46,  1.07s/it, loss=0.2862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 1821/1821 [41:28<00:00,  1.37s/it, loss=0.4466]\n",
      "Validating: 100%|██████████| 322/322 [04:34<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4071\n",
      "Val SMAPE: 44.31%\n",
      "Val MAE: $10.06\n",
      "Val RMSE: $22.31\n",
      "✓ Model saved! Best SMAPE: 44.31%\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10:  72%|███████▏  | 1313/1821 [28:07<09:12,  1.09s/it, loss=0.4030]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 1821/1821 [37:43<00:00,  1.24s/it, loss=0.4375]\n",
      "Validating: 100%|██████████| 322/322 [03:24<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3816\n",
      "Val SMAPE: 45.24%\n",
      "Val MAE: $10.10\n",
      "Val RMSE: $22.61\n",
      "Patience: 1/8\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11:  50%|████▉     | 909/1821 [11:38<09:37,  1.58it/s, loss=0.2866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11:  50%|████▉     | 910/1821 [11:38<08:29,  1.79it/s, loss=0.2866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 1821/1821 [31:25<00:00,  1.04s/it, loss=0.4011]\n",
      "Validating: 100%|██████████| 322/322 [03:16<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3602\n",
      "Val SMAPE: 45.05%\n",
      "Val MAE: $10.65\n",
      "Val RMSE: $22.42\n",
      "Patience: 2/8\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12:  26%|██▌       | 469/1821 [05:50<13:37,  1.65it/s, loss=0.3703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 1821/1821 [22:27<00:00,  1.35it/s, loss=0.2769]\n",
      "Validating: 100%|██████████| 322/322 [03:11<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3444\n",
      "Val SMAPE: 44.30%\n",
      "Val MAE: $10.01\n",
      "Val RMSE: $21.91\n",
      "✓ Model saved! Best SMAPE: 44.30%\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13:   3%|▎         | 57/1821 [00:45<25:06,  1.17it/s, loss=0.2823] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 1821/1821 [22:29<00:00,  1.35it/s, loss=0.3153]\n",
      "Validating: 100%|██████████| 322/322 [03:14<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3249\n",
      "Val SMAPE: 44.53%\n",
      "Val MAE: $10.46\n",
      "Val RMSE: $22.35\n",
      "Patience: 1/8\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14:  71%|███████▏  | 1299/1821 [15:56<05:09,  1.69it/s, loss=0.3848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 1821/1821 [22:19<00:00,  1.36it/s, loss=0.2852]\n",
      "Validating: 100%|██████████| 322/322 [03:14<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3071\n",
      "Val SMAPE: 43.26%\n",
      "Val MAE: $9.73\n",
      "Val RMSE: $21.56\n",
      "✓ Model saved! Best SMAPE: 43.26%\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15:  14%|█▍        | 259/1821 [03:13<15:04,  1.73it/s, loss=0.2631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 279285, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 1821/1821 [22:18<00:00,  1.36it/s, loss=0.2452]\n",
      "Validating: 100%|██████████| 322/322 [03:17<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2937\n",
      "Val SMAPE: 43.17%\n",
      "Val MAE: $9.67\n",
      "Val RMSE: $21.04\n",
      "✓ Model saved! Best SMAPE: 43.17%\n",
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "Best Validation SMAPE: 43.17%\n",
      "============================================================\n",
      "\n",
      "Loading best model for test predictions...\n",
      "\n",
      "Loading test data from dataset/test.csv\n",
      "Test samples: 75000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EfficientNet-BERT Product Price Prediction\")\n",
    "print(\"Alternative Approach for Ensemble\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "gpu_index = 1 if num_gpus > 1 else 0\n",
    "device = torch.device(f\"cuda:{gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(device)}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(1)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(1).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Load data\n",
    "print(f\"\\nLoading training data from {Config.TRAIN_CSV_PATH}\")\n",
    "train_df = pd.read_csv(Config.TRAIN_CSV_PATH)\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "\n",
    "print(\"\\nPrice Statistics:\")\n",
    "print(f\"Min: ${train_df['price'].min():.2f}\")\n",
    "print(f\"Max: ${train_df['price'].max():.2f}\")\n",
    "print(f\"Mean: ${train_df['price'].mean():.2f}\")\n",
    "print(f\"Median: ${train_df['price'].median():.2f}\")\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=Config.VAL_SPLIT, \n",
    "    random_state=Config.SEED\n",
    ")\n",
    "print(f\"\\nTrain set: {len(train_data)} samples\")\n",
    "print(f\"Validation set: {len(val_data)} samples\")\n",
    "\n",
    "print(f\"\\nInitializing tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.TEXT_MODEL)\n",
    "model = DualEncoderPricePredictor().to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params/1e6:.2f}M\")\n",
    "print(f\"Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "print(\"\\nCreating datasets with augmentation...\")\n",
    "train_dataset = ProductPriceDataset(\n",
    "    train_data, tokenizer, Config.TRAIN_IMG_DIR, \n",
    "    mode='train', use_augmentation=Config.USE_AUGMENTATION\n",
    ")\n",
    "val_dataset = ProductPriceDataset(\n",
    "    val_data, tokenizer, Config.TRAIN_IMG_DIR, \n",
    "    mode='train', use_augmentation=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "criterion = SMAPELoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.image_encoder.parameters(), 'lr': Config.LEARNING_RATE_IMG},\n",
    "    {'params': model.text_encoder.parameters(), 'lr': Config.LEARNING_RATE_TEXT},\n",
    "    {'params': model.image_projection.parameters(), 'lr': Config.LEARNING_RATE_HEAD},\n",
    "    {'params': model.text_projection.parameters(), 'lr': Config.LEARNING_RATE_HEAD},\n",
    "    {'params': model.cross_attention.parameters(), 'lr': Config.LEARNING_RATE_HEAD},\n",
    "    {'params': model.fusion_head.parameters(), 'lr': Config.LEARNING_RATE_HEAD}\n",
    "], weight_decay=Config.WEIGHT_DECAY)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() if Config.USE_AMP else None\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nStarting training for {Config.EPOCHS} epochs...\")\n",
    "print(f\"Warmup epochs: {Config.WARMUP_EPOCHS}\")\n",
    "print(f\"Image size: {Config.IMAGE_SIZE}x{Config.IMAGE_SIZE}\")\n",
    "print(f\"Data augmentation: {Config.USE_AUGMENTATION}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "best_smape = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(Config.EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, optimizer, criterion, device, scaler, epoch\n",
    "    )\n",
    "    \n",
    "    val_smape, val_mae, val_rmse = validate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val SMAPE: {val_smape:.2f}%\")\n",
    "    print(f\"Val MAE: ${val_mae:.2f}\")\n",
    "    print(f\"Val RMSE: ${val_rmse:.2f}\")\n",
    "    \n",
    "    scheduler.step(val_smape)\n",
    "    \n",
    "    if val_smape < best_smape:\n",
    "        best_smape = val_smape\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'smape': val_smape\n",
    "        }, Config.MODEL_SAVE_PATH)\n",
    "        print(f\"✓ Model saved! Best SMAPE: {best_smape:.2f}%\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Patience: {patience_counter}/{Config.PATIENCE}\")\n",
    "    \n",
    "    if patience_counter >= Config.PATIENCE:\n",
    "        print(\"\\nEarly stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best Validation SMAPE: {best_smape:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nLoading best model for test predictions...\")\n",
    "checkpoint = torch.load(Config.MODEL_SAVE_PATH, map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"\\nLoading test data from {Config.TEST_CSV_PATH}\")\n",
    "test_df = pd.read_csv(Config.TEST_CSV_PATH)\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ba6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ProductPriceDataset(\n",
    "    test_df, tokenizer, Config.TEST_IMG_DIR, \n",
    "    mode='test', use_augmentation=False, testing=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7755830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/2143 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 9/2143 [00:10<46:05,  1.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129644, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   9%|▉         | 188/2143 [02:48<17:15,  1.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129645, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   9%|▉         | 196/2143 [02:55<17:07,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129669, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  56%|█████▌    | 1196/2143 [17:24<21:38,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 286800, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  56%|█████▌    | 1203/2143 [17:30<13:49,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129661, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  59%|█████▉    | 1274/2143 [18:31<07:55,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129631, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  60%|██████    | 1286/2143 [18:43<07:45,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129619, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  67%|██████▋   | 1432/2143 [20:54<06:15,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129665, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  83%|████████▎ | 1768/2143 [25:57<03:50,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129674, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  87%|████████▋ | 1861/2143 [27:28<06:48,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129666, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  88%|████████▊ | 1876/2143 [27:39<02:40,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129658, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  90%|█████████ | 1937/2143 [28:39<03:50,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129664, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  91%|█████████ | 1944/2143 [28:43<01:41,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not load image for 129649, using blank image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 2143/2143 [31:41<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Submission saved to submission_efficientnet_bert.csv\n",
      "Sample predictions:\n",
      "        sample_id      price\n",
      "0  tensor(100179)  14.132812\n",
      "1  tensor(245611)  18.734375\n",
      "2  tensor(146263)  17.937500\n",
      "3   tensor(95658)   5.554688\n",
      "4   tensor(36806)  22.484375\n",
      "5  tensor(148239)   7.050781\n",
      "6   tensor(92659)   3.765625\n",
      "7    tensor(3780)  12.726562\n",
      "8  tensor(196940)  15.976562\n",
      "9   tensor(20472)   8.320312\n",
      "\n",
      "============================================================\n",
      "Done! Now ensemble with CLIP predictions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating predictions...\")\n",
    "sample_ids, predictions = predict(model, test_loader, device)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'price': predictions\n",
    "})\n",
    "\n",
    "if len(submission_df) != len(test_df):\n",
    "    print(f\"Warning: Prediction count mismatch!\")\n",
    "    print(f\"Expected: {len(test_df)}, Got: {len(submission_df)}\")\n",
    "\n",
    "submission_df.to_csv(Config.SUBMISSION_PATH, index=False)\n",
    "print(f\"\\n✓ Submission saved to {Config.SUBMISSION_PATH}\")\n",
    "print(f\"Sample predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Done! Now ensemble with CLIP predictions!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fc0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Submission saved to submission_efficientnet_bert.csv\n",
      "Sample predictions:\n",
      "   sample_id      price\n",
      "0     100179  14.132812\n",
      "1     245611  18.734375\n",
      "2     146263  17.937500\n",
      "3      95658   5.554688\n",
      "4      36806  22.484375\n",
      "5     148239   7.050781\n",
      "6      92659   3.765625\n",
      "7       3780  12.726562\n",
      "8     196940  15.976562\n",
      "9      20472   8.320312\n",
      "\n",
      "============================================================\n",
      "Done! Now ensemble with CLIP predictions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "sample_ids = [int(id.item()) if hasattr(id, 'item') else int(id) for id in sample_ids]\n",
    "predictions = [float(pred.item()) if hasattr(pred, 'item') else float(pred) for pred in predictions]\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'price': predictions\n",
    "})\n",
    "\n",
    "if len(submission_df) != len(test_df):\n",
    "    print(f\"Warning: Prediction count mismatch!\")\n",
    "    print(f\"Expected: {len(test_df)}, Got: {len(submission_df)}\")\n",
    "\n",
    "submission_df.to_csv(Config.SUBMISSION_PATH, index=False)\n",
    "print(f\"\\n✓ Submission saved to {Config.SUBMISSION_PATH}\")\n",
    "print(f\"Sample predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Done! Now ensemble with CLIP predictions!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
